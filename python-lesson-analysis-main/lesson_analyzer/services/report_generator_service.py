"""ë³´ê³ ì„œ ìƒì„± ë©”ì¸ ì„œë¹„ìŠ¤."""

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Any

from ..models import StudentReport
from ..charts import ChartGenerator
from ..templates import MarkdownTemplateEngine, MarkdownReportTemplate
from ..validators import ReportDataValidator, ReportDataValidationError
from .report_data_processor import ReportDataProcessor
from .report_storage_service import ReportStorageService

logger = logging.getLogger(__name__)


class ReportGeneratorService:
    """ë³´ê³ ì„œ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ ì„œë¹„ìŠ¤."""

    def __init__(self, config_or_output_dir="reports", language: str = 'en', language_analysis_service=None, analysis_data=None):
        """
        ReportGeneratorService ì´ˆê¸°í™”.

        Args:
            config_or_output_dir: AnalysisConfig ê°ì²´ ë˜ëŠ” ë³´ê³ ì„œ ì¶œë ¥ ë””ë ‰í† ë¦¬ ë¬¸ìì—´
            language: ë¶„ì„ ëŒ€ìƒ ì–¸ì–´ ('en': ì˜ì–´, 'ko': í•œêµ­ì–´)
            language_analysis_service: ê¸°ì¡´ LanguageAnalysisService ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì )
            analysis_data: ë¶„ì„ ê²°ê³¼ ë°ì´í„° (ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì¶œìš©)
        """
        # AnalysisConfig ê°ì²´ì¸ì§€ í™•ì¸
        if hasattr(config_or_output_dir, 'output_dir'):
            # AnalysisConfig ê°ì²´ì¸ ê²½ìš°
            self.output_dir = config_or_output_dir.output_dir
            self.language = getattr(config_or_output_dir, 'language', 'en')
            self.config = config_or_output_dir
        else:
            # ë¬¸ìì—´ì¸ ê²½ìš°
            self.output_dir = config_or_output_dir
            self.language = language
            self.config = None
        
        # í•„ìš”í•œ í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤ ìƒì„±
        self.reports_dir = os.path.join(self.output_dir, "reports")
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # results ë””ë ‰í† ë¦¬ëŠ” save_dataê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ìƒì„±
        self.results_dir = os.path.join(self.output_dir, "results")
        should_create_results_dir = False
        if self.config and hasattr(self.config, 'save_data'):
            should_create_results_dir = self.config.save_data
        elif not self.config:
            should_create_results_dir = False
            
        if should_create_results_dir:
            os.makedirs(self.results_dir, exist_ok=True)

        self.chart_generator = ChartGenerator()
        self.template_engine = MarkdownTemplateEngine()
        self.report_templates = MarkdownReportTemplate()
        
        # ê³µìœ ëœ language_analysis_service ì‚¬ìš© (ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€)
        shared_service = None
        # ìš°ì„ ìˆœìœ„: 1) analysis_dataì—ì„œ ì¶”ì¶œ, 2) ì§ì ‘ ì „ë‹¬, 3) configì—ì„œ ì¶”ì¶œ
        if analysis_data and 'language_analysis_service' in analysis_data:
            shared_service = analysis_data['language_analysis_service']
            logger.info("ë¶„ì„ ë°ì´í„°ì—ì„œ ê¸°ì¡´ LanguageAnalysisService ì¬ì‚¬ìš©")
        elif language_analysis_service is not None:
            shared_service = language_analysis_service
            logger.info("ì „ë‹¬ë°›ì€ LanguageAnalysisService ì‚¬ìš©")
        elif self.config and hasattr(self.config, '_shared_language_analysis_service'):
            shared_service = self.config._shared_language_analysis_service
            logger.info("Configì—ì„œ ê³µìœ  LanguageAnalysisService ì‚¬ìš©")
        
        self.data_processor = ReportDataProcessor(language=self.language, language_analysis_service=shared_service)
        self.storage_service = ReportStorageService(self.reports_dir)  # reports í•˜ìœ„ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        self.validator = ReportDataValidator()

    def generate_markdown_report(self, analysis_data: Dict[str, Any]) -> str:
        """
        ë¶„ì„ ë°ì´í„°ë¡œë¶€í„° ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±.

        Args:
            analysis_data: ë¶„ì„ ê²°ê³¼ ë°ì´í„°

        Returns:
            ìƒì„±ëœ ë³´ê³ ì„œ íŒŒì¼ ê²½ë¡œ
        """
        logger.info("ğŸ“‹ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì‹œì‘")

        try:
            # í•™ìƒ ID ëª©ë¡ ì¶”ì¶œ
            student_ids = self._extract_student_ids(analysis_data)
            if not student_ids:
                raise ValueError("ë¶„ì„ ë°ì´í„°ì—ì„œ í•™ìƒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # í™”ì ì´ë¦„ ë§¤í•‘ ê°€ì ¸ì˜¤ê¸°
            final_speaker_mapping = analysis_data.get('final_speaker_mapping', {})
            speaker_names = analysis_data.get('speaker_names', {})
            
            # Step 4: ReportGeneratorService ë°ì´í„° ìˆ˜ì‹  í™•ì¸ ë¡œê¹…
            logger.debug(f"ğŸ” [Step 4] ReportGeneratorServiceì—ì„œ ìˆ˜ì‹ í•œ í™”ì ë°ì´í„°:")
            logger.debug(f"ğŸ” [Step 4] - final_speaker_mapping: {final_speaker_mapping}")
            logger.debug(f"ğŸ” [Step 4] - speaker_names: {speaker_names}")
            logger.debug(f"ğŸ” [Step 4] - student_ids: {student_ids}")
            
            generated_reports = []

            for student_id in student_ids:
                logger.info(f"ğŸ“Š {student_id} í•™ìƒ ë³´ê³ ì„œ ìƒì„± ì¤‘...")

                try:
                    # í•™ìƒë³„ ë³´ê³ ì„œ ë°ì´í„° ìƒì„±
                    logger.debug(f"ë‹¨ê³„ 1: {student_id} ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì‹œì‘")
                    student_report = self.data_processor.create_student_report(analysis_data, student_id)
                    logger.debug(f"ë‹¨ê³„ 1 ì™„ë£Œ: {student_id} ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì„±ê³µ")

                    # ë°ì´í„° ê²€ì¦
                    logger.debug(f"ë‹¨ê³„ 2: {student_id} ë°ì´í„° ê²€ì¦ ì‹œì‘")
                    is_valid, validation_errors = self.validator.validate_student_report(student_report)
                    if not is_valid:
                        logger.warning(f"ë³´ê³ ì„œ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {validation_errors}")
                        # ê²€ì¦ ì‹¤íŒ¨ ì‹œì—ë„ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ë˜ ê²½ê³  í‘œì‹œ
                    logger.debug(f"ë‹¨ê³„ 2 ì™„ë£Œ: {student_id} ë°ì´í„° ê²€ì¦ ì™„ë£Œ")

                    # ì‹¤ì œ í™”ì ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ìš°ì„ ìˆœìœ„: final_speaker_mapping > speaker_names)
                    logger.debug(f"ë‹¨ê³„ 3: {student_id} í™”ì ì´ë¦„ ë§¤í•‘ ì‹œì‘")
                    
                    # Step 4 ìƒì„¸ ë¡œê¹…: í™”ì ì´ë¦„ ë§¤í•‘ ê³¼ì •
                    logger.debug(f"ğŸ” [Step 4] {student_id} í™”ì ì´ë¦„ ë§¤í•‘ ìƒì„¸:")
                    logger.debug(f"ğŸ” [Step 4] - final_speaker_mapping.get('{student_id}'): {final_speaker_mapping.get(student_id)}")
                    logger.debug(f"ğŸ” [Step 4] - speaker_names.get('{student_id}'): {speaker_names.get(student_id)}")
                    
                    actual_name = (
                        final_speaker_mapping.get(student_id) or 
                        speaker_names.get(student_id)
                    )
                    logger.debug(f"ğŸ” [Step 4] - ìµœì¢… ì„ íƒëœ actual_name: {actual_name}")
                    logger.debug(f"ë‹¨ê³„ 3 ì™„ë£Œ: {student_id} í™”ì ì´ë¦„: {actual_name}")

                    # ì˜¬ë°”ë¥¸ report_id ìƒì„± (actual_name ì‚¬ìš©) - ë§ˆí¬ë‹¤ìš´ ìƒì„± ì „ì— ìˆ˜ì •
                    if actual_name and actual_name != 'unknown':
                        timestamp_str = student_report.timestamp.strftime("%Y%m%d%H%M%S")
                        corrected_report_id = f"report_{actual_name}_{timestamp_str}"
                        student_report.report_id = corrected_report_id
                        logger.debug(f"ğŸ” [Step 4] - ë³´ê³ ì„œ ID ìˆ˜ì •: {student_report.report_id}")

                    # ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ìƒì„± (ìˆ˜ì •ëœ report_id ì‚¬ìš©)
                    logger.debug(f"ë‹¨ê³„ 4: {student_id} ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ìƒì„± ì‹œì‘")
                    markdown_content = self._generate_markdown_content(student_report)
                    logger.debug(f"ë‹¨ê³„ 4 ì™„ë£Œ: {student_id} ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ìƒì„± ì„±ê³µ")

                    # ë³´ê³ ì„œ ì €ì¥ (ì‹¤ì œ ì´ë¦„ì„ íŒŒì¼ëª…ì— ì‚¬ìš©)
                    logger.debug(f"ë‹¨ê³„ 5: {student_id} ë³´ê³ ì„œ ì €ì¥ ì‹œì‘")
                    report_path = self.storage_service.save_report(student_id, markdown_content, actual_name)
                    generated_reports.append(report_path)
                    logger.debug(f"ë‹¨ê³„ 5 ì™„ë£Œ: {student_id} ë³´ê³ ì„œ ì €ì¥ ì„±ê³µ")

                    actual_name_display = actual_name or student_id
                    logger.info(f"âœ… {actual_name_display} í•™ìƒ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {report_path}")

                except Exception as student_error:
                    logger.error(f"âŒ {student_id} í•™ìƒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {type(student_error).__name__}: {student_error}")
                    logger.error(f"   ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {repr(student_error)}")
                    # í•œ í•™ìƒì˜ ë³´ê³ ì„œ ìƒì„±ì´ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ í•™ìƒë“¤ì€ ê³„ì† ì²˜ë¦¬
                    continue

            # ë¶„ì„ ë°ì´í„°ë„ JSONìœ¼ë¡œ ì €ì¥ (configì—ì„œ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
            should_save_analysis_data = False
            if self.config and hasattr(self.config, 'save_analysis_data'):
                should_save_analysis_data = self.config.save_analysis_data
            elif not self.config:
                # configê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”
                should_save_analysis_data = False
            
            if should_save_analysis_data:
                try:
                    logger.debug("ë¶„ì„ ë°ì´í„° JSON ì €ì¥ ì‹œì‘")
                    self.save_analysis_data(analysis_data)
                    logger.debug("ë¶„ì„ ë°ì´í„° JSON ì €ì¥ ì™„ë£Œ")
                except Exception as save_error:
                    logger.error(f"âŒ ë¶„ì„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {type(save_error).__name__}: {save_error}")
                    # ë¶„ì„ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ëŠ” ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
            else:
                logger.debug("ë¶„ì„ ë°ì´í„° JSON ì €ì¥ ë¹„í™œì„±í™”ë¨")

            # ì²« ë²ˆì§¸ ë³´ê³ ì„œ ê²½ë¡œ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            return generated_reports[0] if generated_reports else ""

        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    def _sanitize_unicode_data(self, data):
        """
        ë°ì´í„°ì—ì„œ surrogate ë¬¸ì ë° problematic Unicode ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            data: ì •ì œí•  ë°ì´í„° (dict, list, str, ë˜ëŠ” ê¸°íƒ€)
            
        Returns:
            ì •ì œëœ ë°ì´í„°
        """
        import unicodedata
        
        if isinstance(data, dict):
            return {key: self._sanitize_unicode_data(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._sanitize_unicode_data(item) for item in data]
        elif isinstance(data, tuple):
            return tuple(self._sanitize_unicode_data(item) for item in data)
        elif isinstance(data, str):
            try:
                # 1. Unicode ì •ê·œí™”
                normalized = unicodedata.normalize('NFC', data)
                
                # 2. UTF-8 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
                test_encoded = normalized.encode('utf-8')
                test_decoded = test_encoded.decode('utf-8')
                return test_decoded
                
            except (UnicodeEncodeError, UnicodeDecodeError):
                # 3. ì•ˆì „í•œ ë¬¸ìë§Œ ì¶”ì¶œ (surrogate ë¬¸ì ì œê±°)
                safe_chars = []
                for char in data:
                    try:
                        # surrogate ë¬¸ì ì œì™¸
                        if 0xD800 <= ord(char) <= 0xDFFF:
                            continue
                        # UTF-8 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
                        char.encode('utf-8')
                        safe_chars.append(char)
                    except (UnicodeEncodeError, ValueError):
                        # ë¬¸ì œê°€ ìˆëŠ” ë¬¸ìëŠ” ì œì™¸
                        continue
                
                cleaned_text = ''.join(safe_chars)
                logger.warning(f"Unicode ì •ì œ: ë¬¸ì œê°€ ìˆëŠ” ë¬¸ì ì œê±°ë¨ (ì›ë³¸ ê¸¸ì´: {len(data)}, ì •ì œ í›„: {len(cleaned_text)})")
                return cleaned_text
        else:
            return data

    def _filter_non_serializable_objects(self, data):
        """
        JSON serializableí•˜ì§€ ì•Šì€ ê°ì²´ë“¤ì„ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            data: í•„í„°ë§í•  ë°ì´í„°
            
        Returns:
            JSON serializableí•œ ë°ì´í„°
        """
        import json
        from ..services.language_analysis_service import LanguageAnalysisService
        
        if isinstance(data, dict):
            filtered_dict = {}
            for key, value in data.items():
                # LanguageAnalysisService ê°™ì€ ì„œë¹„ìŠ¤ ê°ì²´ ì œì™¸
                if key == 'language_analysis_service' or isinstance(value, LanguageAnalysisService):
                    logger.debug(f"JSON ì €ì¥ì—ì„œ ì œì™¸: {key} ({type(value)})")
                    continue
                
                # ì¬ê·€ì ìœ¼ë¡œ í•„í„°ë§
                try:
                    filtered_value = self._filter_non_serializable_objects(value)
                    # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                    json.dumps(filtered_value, default=str)
                    filtered_dict[key] = filtered_value
                except (TypeError, ValueError) as e:
                    logger.debug(f"JSON ì§ë ¬í™” ì‹¤íŒ¨ë¡œ ì œì™¸: {key} ({type(value)}): {e}")
                    continue
            return filtered_dict
            
        elif isinstance(data, (list, tuple)):
            filtered_list = []
            for item in data:
                try:
                    filtered_item = self._filter_non_serializable_objects(item)
                    # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                    json.dumps(filtered_item, default=str)
                    filtered_list.append(filtered_item)
                except (TypeError, ValueError):
                    logger.debug(f"JSON ì§ë ¬í™” ì‹¤íŒ¨ë¡œ ë¦¬ìŠ¤íŠ¸ í•­ëª© ì œì™¸: {type(item)}")
                    continue
            return type(data)(filtered_list) if isinstance(data, tuple) else filtered_list
            
        else:
            # ê¸°ë³¸ íƒ€ì…ë“¤ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
            try:
                json.dumps(data, default=str)
                return data
            except (TypeError, ValueError):
                logger.debug(f"JSON ì§ë ¬í™” ì‹¤íŒ¨ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©: {type(data)}")
                return str(data)

    def save_analysis_data(self, analysis_data: Dict[str, Any]) -> None:
        """
        ë¶„ì„ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥.

        Args:
            analysis_data: ì €ì¥í•  ë¶„ì„ ë°ì´í„°
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"analysis_data_{timestamp}.json"
        filepath = os.path.join(self.results_dir, filename)  # results ë””ë ‰í† ë¦¬ì— ì €ì¥

        # JSON serializableí•˜ì§€ ì•Šì€ ê°ì²´ í•„í„°ë§
        try:
            filtered_data = self._filter_non_serializable_objects(analysis_data)
            logger.debug("Non-serializable ê°ì²´ í•„í„°ë§ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ê°ì²´ í•„í„°ë§ ì‹¤íŒ¨, ì›ë³¸ ë°ì´í„° ì‚¬ìš©: {e}")
            filtered_data = analysis_data

        # ë°ì´í„° ì •ì œ (surrogate ë¬¸ì ì œê±°)
        try:
            sanitized_data = self._sanitize_unicode_data(filtered_data)
            logger.debug("ë¶„ì„ ë°ì´í„° Unicode ì •ì œ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Unicode ì •ì œ ì‹¤íŒ¨, í•„í„°ë§ëœ ë°ì´í„° ì‚¬ìš©: {e}")
            sanitized_data = filtered_data

        class DateTimeEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, datetime):
                    return obj.isoformat()
                # dataclass ê°ì²´ ì²˜ë¦¬
                if hasattr(obj, '__dataclass_fields__'):
                    return {field: getattr(obj, field) for field in obj.__dataclass_fields__}
                # numpy íƒ€ì… ì²˜ë¦¬
                if hasattr(obj, 'tolist'):
                    return obj.tolist()
                # numpy scalar ì²˜ë¦¬
                if hasattr(obj, 'item'):
                    return obj.item()
                return super().default(obj)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(sanitized_data, f, cls=DateTimeEncoder, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ ë¶„ì„ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        except UnicodeEncodeError as e:
            # fallback: ensure_ascii=Trueë¡œ ì¬ì‹œë„
            logger.warning(f"UTF-8 ì¸ì½”ë”© ì‹¤íŒ¨, ASCII ëª¨ë“œë¡œ ì¬ì‹œë„: {e}")
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(sanitized_data, f, cls=DateTimeEncoder, ensure_ascii=True, indent=2)
            logger.info(f"ğŸ“ ë¶„ì„ ë°ì´í„° ì €ì¥ ì™„ë£Œ (ASCII ëª¨ë“œ): {filepath}")

    def _extract_student_ids(self, analysis_data: Dict[str, Any]) -> list:
        """ë¶„ì„ ë°ì´í„°ì—ì„œ í•™ìƒ ID ëª©ë¡ ì¶”ì¶œ."""
        student_ids = set()

        # final_speaker_mappingì—ì„œ ì‹¤ì œ ë§¤í•‘ëœ í™”ìë§Œ ì¶”ì¶œ
        final_mapping = analysis_data.get('final_speaker_mapping', {})
        speaker_names = analysis_data.get('speaker_names', {})
        
        # ì‹¤ì œ ì´ë¦„ì´ ë§¤í•‘ëœ í™”ìë“¤ë§Œ ì¶”ì¶œ
        for speaker_id, name in final_mapping.items():
            # ì„ ìƒë‹˜/ë©´ì ‘ê´€ì€ ì œì™¸í•˜ê³  í•™ìƒë§Œ í¬í•¨
            if name and 'ì„ ìƒë‹˜' not in name and 'teacher' not in name.lower() and 'ë©´ì ‘ê´€' not in name:
                if speaker_id.startswith('person_'):
                    student_ids.add(speaker_id)
        
        # final_mappingì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆë‹¤ë©´ speaker_namesì—ì„œ ì¶”ì¶œ
        if not student_ids and speaker_names:
            for speaker_id, name in speaker_names.items():
                if name and 'ì„ ìƒë‹˜' not in name and 'teacher' not in name.lower() and 'ë©´ì ‘ê´€' not in name:
                    if speaker_id.startswith('person_'):
                        student_ids.add(speaker_id)
        
        # ì—¬ì „íˆ í•™ìƒì´ ì—†ë‹¤ë©´ speaker_identificationì—ì„œ ì‹¤ì œ ì „ì‚¬ê°€ ìˆëŠ” í™”ìë§Œ ì¶”ì¶œ
        if not student_ids and 'speaker_identification' in analysis_data:
            speaker_data = analysis_data['speaker_identification']
            if 'updated_transcription' in speaker_data:
                for speaker_id, transcripts in speaker_data['updated_transcription'].items():
                    # ì‹¤ì œ ì „ì‚¬ ë°ì´í„°ê°€ ìˆê³ , person_ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ë§Œ
                    if (speaker_id.startswith('person_') and 
                        transcripts and 
                        len(transcripts) > 0 and 
                        speaker_id != 'person_ids'):  # person_idsëŠ” ì œì™¸
                        # í•´ë‹¹ í™”ìì˜ ì‹¤ì œ ì´ë¦„ í™•ì¸
                        actual_name = (final_mapping.get(speaker_id) or 
                                     speaker_names.get(speaker_id) or '')
                        # ì„ ìƒë‹˜/ë©´ì ‘ê´€ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
                        if 'ì„ ìƒë‹˜' not in actual_name and 'teacher' not in actual_name.lower() and 'ë©´ì ‘ê´€' not in actual_name:
                            student_ids.add(speaker_id)

        logger.info(f"ì¶”ì¶œëœ í•™ìƒ ID ëª©ë¡: {list(student_ids)}")
        return list(student_ids)

    def _generate_markdown_content(self, student_report: StudentReport) -> str:
        """í•™ìƒ ë³´ê³ ì„œë¡œë¶€í„° ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  ìƒì„±."""
        # í…œí”Œë¦¿ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        context = self._prepare_template_context(student_report)

        # ì°¨íŠ¸ ìƒì„±
        charts = self._generate_charts(student_report)
        context.update(charts)

        # ê° ì„¹ì…˜ë³„ ë§ˆí¬ë‹¤ìš´ ìƒì„±
        sections = []

        # í—¤ë” ì„¹ì…˜
        sections.append(self.template_engine.render_template(
            self.report_templates.HEADER_TEMPLATE, context
        ))

        # ì°¸ì—¬ë„ ì„¹ì…˜
        if student_report.participation:
            sections.append(self.template_engine.render_template(
                self.report_templates.PARTICIPATION_TEMPLATE, context
            ))

        # ìœ ì°½ì„± ì„¹ì…˜
        if student_report.fluency:
            sections.append(self.template_engine.render_template(
                self.report_templates.FLUENCY_TEMPLATE, context
            ))

        # ì–´íœ˜ ì„¹ì…˜
        if student_report.vocabulary:
            sections.append(self.template_engine.render_template(
                self.report_templates.VOCABULARY_TEMPLATE, context
            ))

        # ì£¼ì œ ì¹œë°€ë„ ì„¹ì…˜
        if student_report.topic_familiarity:
            sections.append(self.template_engine.render_template(
                self.report_templates.TOPIC_FAMILIARITY_TEMPLATE, context
            ))

        # ë¬¸ë²• ì„¹ì…˜
        if student_report.grammar:
            sections.append(self.template_engine.render_template(
                self.report_templates.GRAMMAR_TEMPLATE, context
            ))

        # ë°œìŒ ì„¹ì…˜
        if student_report.pronunciation:
            sections.append(self.template_engine.render_template(
                self.report_templates.PRONUNCIATION_TEMPLATE, context
            ))

        # ì§„í–‰ ìƒí™© ì„¹ì…˜
        if student_report.progress:
            sections.append(self.template_engine.render_template(
                self.report_templates.PROGRESS_TEMPLATE, context
            ))

        # ìš”ì•½ ì„¹ì…˜
        sections.append(self.template_engine.render_template(
            self.report_templates.SUMMARY_TEMPLATE, context
        ))

        return "\n".join(sections)

    def _prepare_template_context(self, student_report: StudentReport) -> Dict[str, Any]:
        """í…œí”Œë¦¿ ë Œë”ë§ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„."""
        context = {
            'student_profile': student_report.student_profile,
            'participation': student_report.participation,
            'fluency': student_report.fluency,
            'vocabulary': student_report.vocabulary,
            'grammar': student_report.grammar,
            'pronunciation': student_report.pronunciation,
            'topic_familiarity': student_report.topic_familiarity,
            'progress': student_report.progress,
            'timestamp': student_report.timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            'report_id': student_report.report_id
        }

        # ì„¸ì…˜ ì •ë³´ ì¡°ê±´ë¶€ ì¶”ê°€ (session_idê°€ Noneì´ê±°ë‚˜ 'unknown'ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        if (student_report.session_info and 
            student_report.session_info.session_id and 
            student_report.session_info.session_id != 'unknown'):
            context['session_info'] = student_report.session_info

        # ì¶”ê°€ ê³„ì‚°ëœ í•„ë“œë“¤
        if student_report.session_info:
            context['session_date'] = self.template_engine.format_date(student_report.session_info.date)
            context['session_duration'] = round(student_report.session_info.duration / 60, 1)

        if student_report.participation:
            total_time = student_report.session_info.duration if student_report.session_info else 1
            # division by zero ë°©ì§€
            if total_time > 0:
                speaking_percentage = round(
                    (student_report.participation.total_speaking_time / total_time) * 100, 1
                )
            else:
                speaking_percentage = 0
            context['speaking_percentage'] = speaking_percentage
            context['total_speaking_time'] = round(student_report.participation.total_speaking_time / 60, 1)

        # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì˜ í•„ë“œë“¤ í¬ë§·íŒ…
        if student_report.student_profile.learning_goals:
            context['learning_goals'] = self.template_engine.format_list(
                student_report.student_profile.learning_goals
            )

        if student_report.fluency and student_report.fluency.improvement_suggestions:
            context['improvement_suggestions'] = self.template_engine.format_list(
                student_report.fluency.improvement_suggestions
            )

        # ì£¼ì œ ì¹œë°€ë„ ì„¹ì…˜ ì¶”ê°€ ë°ì´í„°
        if student_report.topic_familiarity:
            if student_report.topic_familiarity.topic_keywords:
                context['topic_keywords_list'] = self.template_engine.format_list(
                    student_report.topic_familiarity.topic_keywords
                )
            else:
                context['topic_keywords_list'] = "ì£¼ì œ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            if student_report.topic_familiarity.student_keywords:
                context['used_keywords_list'] = self.template_engine.format_list(
                    student_report.topic_familiarity.student_keywords[:10]  # ìƒìœ„ 10ê°œë§Œ
                )
            else:
                context['used_keywords_list'] = "ì‚¬ìš©ëœ í‚¤ì›Œë“œê°€ ì‹ë³„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # ìš”ì•½ ì„¹ì…˜ ë°ì´í„° ìƒì„±
        context.update(self._generate_summary_data(student_report))

        return context

    def _generate_summary_data(self, student_report: StudentReport) -> Dict[str, str]:
        """ìš”ì•½ ì„¹ì…˜ì„ ìœ„í•œ ë°ì´í„° ìƒì„±."""
        summary_data = {}
        
        # ì£¼ìš” ì„±ê³¼ ë¶„ì„
        achievements = self._analyze_main_achievements(student_report)
        summary_data['main_achievements'] = self.template_engine.format_list(achievements) if achievements else "ì´ë²ˆ ì„¸ì…˜ì—ì„œì˜ ì„±ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        
        # ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ ë¶„ì„
        improvement_areas = self._analyze_improvement_areas(student_report)
        summary_data['improvement_areas'] = self.template_engine.format_list(improvement_areas) if improvement_areas else "í˜„ì¬ ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•œ ìˆ˜ì¤€ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
        
        # ë‹¤ìŒ ì„¸ì…˜ ì¶”ì²œì‚¬í•­
        recommendations = self._generate_next_session_recommendations(student_report)
        summary_data['next_session_recommendations'] = self.template_engine.format_list(recommendations)
        
        return summary_data
    
    def _analyze_main_achievements(self, student_report: StudentReport) -> List[str]:
        """ì£¼ìš” ì„±ê³¼ ë¶„ì„."""
        achievements = []
        
        achievements.extend(self._analyze_participation_achievements(student_report))
        achievements.extend(self._analyze_fluency_achievements(student_report))
        achievements.extend(self._analyze_vocabulary_achievements(student_report))
        achievements.extend(self._analyze_grammar_achievements(student_report))
        
        return achievements
    
    def _analyze_participation_achievements(self, student_report: StudentReport) -> List[str]:
        """ì°¸ì—¬ë„ ì„±ê³¼ ë¶„ì„."""
        achievements = []
        
        if student_report.participation:
            participation_score = student_report.participation.participation_score
            if participation_score >= 80:
                achievements.append(f"ìš°ìˆ˜í•œ ì°¸ì—¬ë„ ({participation_score:.0f}/100) - ì ê·¹ì ìœ¼ë¡œ ìˆ˜ì—…ì— ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤.")
            elif participation_score >= 60:
                achievements.append(f"ì–‘í˜¸í•œ ì°¸ì—¬ë„ ({participation_score:.0f}/100) - ìˆ˜ì—…ì— ì°¸ì—¬í•˜ëŠ” ëª¨ìŠµì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")
            
            if student_report.participation.speaking_turns > 10:
                achievements.append(f"í™œë°œí•œ ë°œí™” í™œë™ - ì´ {student_report.participation.speaking_turns}íšŒ ë°œí™”í–ˆìŠµë‹ˆë‹¤.")
        
        return achievements
    
    def _analyze_fluency_achievements(self, student_report: StudentReport) -> List[str]:
        """ìœ ì°½ì„± ì„±ê³¼ ë¶„ì„."""
        achievements = []
        
        if student_report.fluency:
            fluency_score = student_report.fluency.overall_score
            if fluency_score >= 75:
                achievements.append(f"ì–‘í˜¸í•œ ìœ ì°½ì„± ìˆ˜ì¤€ ({fluency_score:.0f}/100) - ìì—°ìŠ¤ëŸ¬ìš´ ë°œí™”ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.")
            
            if student_report.fluency.speech_rate > 100:
                achievements.append(f"ì ì ˆí•œ ë§í•˜ê¸° ì†ë„ ({student_report.fluency.speech_rate:.0f} ë‹¨ì–´/ë¶„)ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.")
        
        return achievements
    
    def _analyze_vocabulary_achievements(self, student_report: StudentReport) -> List[str]:
        """ì–´íœ˜ ì„±ê³¼ ë¶„ì„."""
        achievements = []
        
        if student_report.vocabulary:
            vocab = student_report.vocabulary
            if vocab.unique_word_count > 50:
                achievements.append(f"ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš© - {vocab.unique_word_count}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.")
            
            if vocab.type_token_ratio > 0.6:
                achievements.append(f"ìš°ìˆ˜í•œ ì–´íœ˜ ë‹¤ì–‘ì„± (TTR: {vocab.type_token_ratio:.3f}) - ë°˜ë³µ ì‚¬ìš©ì„ í”¼í•˜ê³  ë‹¤ì–‘í•œ í‘œí˜„ì„ í™œìš©í–ˆìŠµë‹ˆë‹¤.")
            
            # ê³ ê¸‰ ì–´íœ˜ ì‚¬ìš©
            if hasattr(vocab, 'difficulty_percentages') and vocab.difficulty_percentages:
                if vocab.difficulty_percentages.get('advanced', 0) > 15:
                    achievements.append(f"ê³ ê¸‰ ì–´íœ˜ í™œìš© - ì „ì²´ ì–´íœ˜ì˜ {vocab.difficulty_percentages['advanced']:.1f}%ê°€ ê³ ê¸‰ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        
        return achievements
    
    def _analyze_grammar_achievements(self, student_report: StudentReport) -> List[str]:
        """ë¬¸ë²• ì„±ê³¼ ë¶„ì„."""
        achievements = []
        
        if student_report.grammar:
            grammar = student_report.grammar
            if grammar.accuracy_score >= 80:
                achievements.append(f"ìš°ìˆ˜í•œ ë¬¸ë²• ì •í™•ë„ ({grammar.accuracy_score:.0f}/100) - ì •í™•í•œ ë¬¸ë²• ì‚¬ìš©ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")
            elif grammar.total_errors <= 3:
                achievements.append(f"ì–‘í˜¸í•œ ë¬¸ë²• ì‚¬ìš© - ì „ì²´ {grammar.total_errors}ê°œì˜ ì˜¤ë¥˜ë¡œ ë¹„êµì  ì •í™•í–ˆìŠµë‹ˆë‹¤.")
        
        return achievements
    
    def _analyze_improvement_areas(self, student_report: StudentReport) -> List[str]:
        """ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ ë¶„ì„."""
        improvement_areas = []
        
        improvement_areas.extend(self._analyze_participation_improvements(student_report))
        improvement_areas.extend(self._analyze_fluency_improvements(student_report))
        improvement_areas.extend(self._analyze_vocabulary_improvements(student_report))
        improvement_areas.extend(self._analyze_grammar_improvements(student_report))
        
        return improvement_areas
    
    def _analyze_participation_improvements(self, student_report: StudentReport) -> List[str]:
        """ì°¸ì—¬ë„ ê°œì„ ì  ë¶„ì„."""
        improvements = []
        
        if student_report.participation and student_report.participation.participation_score < 60:
            improvements.append(f"ì°¸ì—¬ë„ í–¥ìƒ í•„ìš” ({student_report.participation.participation_score:.0f}/100) - ë” ì ê·¹ì ì¸ ìˆ˜ì—… ì°¸ì—¬ê°€ ê¶Œì¥ë©ë‹ˆë‹¤.")
        
        return improvements
    
    def _analyze_fluency_improvements(self, student_report: StudentReport) -> List[str]:
        """ìœ ì°½ì„± ê°œì„ ì  ë¶„ì„."""
        improvements = []
        
        if student_report.fluency:
            fluency = student_report.fluency
            if fluency.overall_score < 70:
                improvements.append(f"ìœ ì°½ì„± ê°œì„  í•„ìš” ({fluency.overall_score:.0f}/100) - ë” ìì—°ìŠ¤ëŸ¬ìš´ ë°œí™” ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            if fluency.filled_pauses > 10:
                improvements.append(f"ì±„ì›€ë§ ì‚¬ìš© ì¤„ì´ê¸° - 'um', 'uh' ë“±ì˜ ì±„ì›€ë§ì„ {fluency.filled_pauses}íšŒ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.")
        
        return improvements
    
    def _analyze_vocabulary_improvements(self, student_report: StudentReport) -> List[str]:
        """ì–´íœ˜ ê°œì„ ì  ë¶„ì„."""
        improvements = []
        
        if student_report.vocabulary:
            vocab = student_report.vocabulary
            if vocab.type_token_ratio < 0.4:
                improvements.append(f"ì–´íœ˜ ë‹¤ì–‘ì„± í–¥ìƒ í•„ìš” (TTR: {vocab.type_token_ratio:.3f}) - ë” ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            if hasattr(vocab, 'difficulty_percentages') and vocab.difficulty_percentages:
                if vocab.difficulty_percentages.get('basic', 0) > 70:
                    improvements.append(f"ì¤‘ê¸‰ ì´ìƒ ì–´íœ˜ ì‚¬ìš© ì¦ê°€ í•„ìš” - í˜„ì¬ ê¸°ì´ˆ ì–´íœ˜ ì‚¬ìš© ë¹„ìœ¨ì´ {vocab.difficulty_percentages['basic']:.1f}%ì…ë‹ˆë‹¤.")
        
        return improvements
    
    def _analyze_grammar_improvements(self, student_report: StudentReport) -> List[str]:
        """ë¬¸ë²• ê°œì„ ì  ë¶„ì„."""
        improvements = []
        
        if student_report.grammar:
            grammar = student_report.grammar
            if grammar.accuracy_score < 70:
                improvements.append(f"ë¬¸ë²• ì •í™•ë„ í–¥ìƒ í•„ìš” ({grammar.accuracy_score:.0f}/100) - ê¸°ë³¸ ë¬¸ë²• ì‚¬ìš©ì— ë” ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            if grammar.total_errors > 5:
                improvements.append(f"ì˜¤ë¥˜ ë¹ˆë„ ê°ì†Œ í•„ìš” - ì´ {grammar.total_errors}ê°œì˜ ë¬¸ë²• ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return improvements
    
    def _generate_next_session_recommendations(self, student_report: StudentReport) -> List[str]:
        """ë‹¤ìŒ ì„¸ì…˜ ì¶”ì²œì‚¬í•­ ìƒì„±."""
        recommendations = []
        
        recommendations.extend(self._generate_participation_recommendations(student_report))
        recommendations.extend(self._generate_fluency_recommendations(student_report))
        recommendations.extend(self._generate_vocabulary_recommendations(student_report))
        recommendations.extend(self._generate_grammar_recommendations(student_report))
        
        # ê¸°ë³¸ ì¶”ì²œì‚¬í•­
        if not recommendations:
            recommendations.extend([
                "í˜„ì¬ ìˆ˜ì¤€ ìœ ì§€ - ê¾¸ì¤€í•œ ì—°ìŠµì„ í†µí•´ í˜„ì¬ì˜ ì¢‹ì€ ìˆ˜ì¤€ì„ ê³„ì† ìœ ì§€í•´ë‚˜ê°€ì„¸ìš”.",
                "ë‹¤ì–‘í•œ ì£¼ì œ ëŒ€í™” - ì—¬ëŸ¬ ì£¼ì œì— ëŒ€í•´ ëŒ€í™”í•´ë³´ë©° í‘œí˜„ë ¥ì„ ë”ìš± ë°œì „ì‹œì¼œë³´ì„¸ìš”.",
                "ìì‹ ê° ìˆê²Œ ë§í•˜ê¸° - ì‹¤ìˆ˜ë¥¼ ë‘ë ¤ì›Œí•˜ì§€ ë§ê³  ì ê·¹ì ìœ¼ë¡œ ì˜ì‚¬í‘œí˜„ì„ í•´ë³´ì„¸ìš”."
            ])
        
        return recommendations
    
    def _generate_participation_recommendations(self, student_report: StudentReport) -> List[str]:
        """ì°¸ì—¬ë„ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±."""
        recommendations = []
        
        if student_report.participation:
            if student_report.participation.questions_asked < 3:
                recommendations.append("ë” ë§ì€ ì§ˆë¬¸í•˜ê¸° - ê¶ê¸ˆí•œ ì ì´ë‚˜ í™•ì¸í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
            
            if student_report.participation.participation_score < 70:
                recommendations.append("ë°œí™” ê¸°íšŒ ëŠ˜ë¦¬ê¸° - ì„ ìƒë‹˜ì˜ ì§ˆë¬¸ì— ë” ê¸¸ê³  ìì„¸í•œ ë‹µë³€ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
        
        return recommendations
    
    def _generate_fluency_recommendations(self, student_report: StudentReport) -> List[str]:
        """ìœ ì°½ì„± ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±."""
        recommendations = []
        
        if student_report.fluency:
            if student_report.fluency.overall_score < 75:
                recommendations.append("ìœ ì°½ì„± ì—°ìŠµ - ì¼ìƒ ëŒ€í™” ì—°ìŠµì´ë‚˜ ì½ê¸° ì—°ìŠµì„ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ ë°œí™”ë¥¼ ì—°ìŠµí•˜ì„¸ìš”.")
            
            if student_report.fluency.filled_pauses > 8:
                recommendations.append("ì±„ì›€ë§ ì¤„ì´ê¸° ì—°ìŠµ - ë§í•˜ê¸° ì „ ì ì‹œ ìƒê°í•˜ëŠ” ì‹œê°„ì„ ê°–ê³  ì²œì²œíˆ ë§í•˜ëŠ” ì—°ìŠµì„ í•´ë³´ì„¸ìš”.")
        
        return recommendations
    
    def _generate_vocabulary_recommendations(self, student_report: StudentReport) -> List[str]:
        """ì–´íœ˜ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±."""
        recommendations = []
        
        if student_report.vocabulary:
            vocab = student_report.vocabulary
            if vocab.type_token_ratio < 0.5:
                recommendations.append("ì–´íœ˜ ë‹¤ì–‘ì„± í™•ì¥ - ë™ì˜ì–´ ì‚¬ì „ì„ í™œìš©í•´ ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ì–‘í•œ í‘œí˜„ì„ í•™ìŠµí•´ë³´ì„¸ìš”.")
            
            if hasattr(vocab, 'difficulty_percentages') and vocab.difficulty_percentages:
                if vocab.difficulty_percentages.get('intermediate', 0) < 30:
                    recommendations.append("ì¤‘ê¸‰ ì–´íœ˜ í•™ìŠµ - B1-B2 ë ˆë²¨ì˜ ì–´íœ˜ë¥¼ ì¼ì¼ 10ê°œì”© í•™ìŠµí•˜ì—¬ í‘œí˜„ë ¥ì„ ë†’ì—¬ë³´ì„¸ìš”.")
        
        return recommendations
    
    def _generate_grammar_recommendations(self, student_report: StudentReport) -> List[str]:
        """ë¬¸ë²• ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±."""
        recommendations = []
        
        if student_report.grammar and student_report.grammar.total_errors > 3:
            # ê°€ì¥ ë¹ˆë²ˆí•œ ì˜¤ë¥˜ ìœ í˜•ì— ëŒ€í•œ êµ¬ì²´ì  ì¶”ì²œ
            if student_report.grammar.grammar_errors:
                error_types = {}
                # ëª¨ë“  ì˜¤ë¥˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ ìœ í˜•ë³„ ê°œìˆ˜ ì§‘ê³„
                for error in student_report.grammar.grammar_errors:
                    error_type = error.get('error_type', 'grammar')
                    error_types[error_type] = error_types.get(error_type, 0) + 1
                
                # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 3ê°œ ìœ í˜•ë§Œ ì¶”ì²œì— í¬í•¨
                sorted_error_types = sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:3]
                for error_type, count in sorted_error_types:
                    recommendations.append(f"{error_type.title()} ì˜¤ë¥˜ ì§‘ì¤‘ ì—°ìŠµ - ì´ ìœ í˜•ì˜ ì˜¤ë¥˜ê°€ {count}íšŒ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return recommendations

    def _generate_charts(self, student_report: StudentReport) -> Dict[str, str]:
        """í•™ìƒ ë³´ê³ ì„œì— ëŒ€í•œ ì°¨íŠ¸ë“¤ ìƒì„±."""
        charts = {}

        try:
            # ì°¸ì—¬ë„ ì°¨íŠ¸
            if student_report.participation:
                engagement_data = {
                    'ë°œí™” ì‹œê°„(ë¶„)': student_report.participation.total_speaking_time / 60,
                    'ë°œí™” í„´ ìˆ˜': student_report.participation.speaking_turns,
                    'ì§ˆë¬¸ ìˆ˜': student_report.participation.questions_asked,
                    'ì‘ë‹µ ìˆ˜': student_report.participation.responses_given
                }
                charts['engagement_chart'] = self.chart_generator.generate_bar_chart(
                    engagement_data,
                    title='ì°¸ì—¬ë„ ë©”íŠ¸ë¦­',
                    xlabel='í•­ëª©',
                    ylabel='ìˆ˜ì¹˜'
                )

            # ìœ ì°½ì„± ì°¨íŠ¸
            if student_report.fluency:
                fluency_data = {
                    'ì „ì²´ ì ìˆ˜': student_report.fluency.overall_score,
                    'ë¦¬ë“¬ ì ìˆ˜': student_report.fluency.rhythm_score,
                    'ì†ë„ ì ìˆ˜': student_report.fluency.pace_score,
                    'ì¼ê´€ì„± ì ìˆ˜': student_report.fluency.consistency_score
                }
                charts['fluency_chart'] = self.chart_generator.generate_radar_chart(
                    fluency_data,
                    title='ìœ ì°½ì„± ë¶„ì„'
                )

            # ë‹¨ì–´ ë¹ˆë„ ì°¨íŠ¸ (ìƒìœ„ 10ê°œë§Œ)
            if student_report.vocabulary and student_report.vocabulary.word_frequency:
                top_words = dict(
                    sorted(student_report.vocabulary.word_frequency.items(),
                           key=lambda x: x[1], reverse=True)[:10]
                )
                charts['word_frequency_chart'] = self.chart_generator.generate_bar_chart(
                    top_words,
                    title='ìì£¼ ì‚¬ìš©í•œ ë‹¨ì–´ (ìƒìœ„ 10ê°œ)',
                    xlabel='ë‹¨ì–´',
                    ylabel='ë¹ˆë„'
                )

            # ì£¼ì œ ì¹œë°€ë„ ì°¨íŠ¸
            if student_report.topic_familiarity:
                topic_data = {
                    'ì¹œë°€ë„ ì ìˆ˜': student_report.topic_familiarity.familiarity_score * 100,
                    'ì£¼ì œ ìœ ì‚¬ë„': student_report.topic_familiarity.semantic_similarity * 100,
                    'ì£¼ì œ ì°¸ì—¬ë„': student_report.topic_familiarity.topic_engagement * 100,
                    'í‚¤ì›Œë“œ ë§¤ì¹­': student_report.topic_familiarity.keyword_match_ratio * 100
                }
                charts['topic_familiarity_chart'] = self.chart_generator.generate_radar_chart(
                    topic_data,
                    title='ì£¼ì œ ì¹œë°€ë„ ë¶„ì„'
                )

            # ì§„í–‰ ìƒí™© ì°¨íŠ¸
            if student_report.progress:
                progress_data = {
                    'ì „ì²´': student_report.progress.overall_progress,
                    'ìœ ì°½ì„±': student_report.progress.fluency_progress,
                    'ì–´íœ˜': student_report.progress.vocabulary_progress,
                    'ë¬¸ë²•': student_report.progress.grammar_progress,
                    'ë°œìŒ': student_report.progress.pronunciation_progress
                }
                charts['progress_chart'] = self.chart_generator.generate_radar_chart(
                    progress_data,
                    title='ì§„í–‰ ìƒí™© ë¶„ì„'
                )

        except Exception as e:
            logger.warning(f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        return charts 